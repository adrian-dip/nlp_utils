def layerwise_lr(model, lr):
  
  # get a list of the model's layers
  
  layers = []
  for idx, (name, param) in enumerate(model.named_parameters()):
      layers.append(name)


  # reverse Since we want to decrease the lr (The list goes from the bottom layers to the top). 
  
  layers.reverse()

  lr_fn = lr
  ξ = 0.95 # hyperparameter by which to decrease (lr * epsilon = 5%)

  parameters = []
  prev_group_name = layers[2].split('.')[2] # Spare the first layer from decreasing (see readme for an example)

  # store params & learning rates
  for idx, name in enumerate(layers):

      # parameter group name
      cur_group_name = name.split('.')[2]

      # update learning rate
      if cur_group_name != prev_group_name: # Spare the first layer from decreasing (see readme for an example)
          lr_fn *= ξ
      prev_group_name = cur_group_name

      # append layer parameters
      parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],
                      'lr':     lr}]
